{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8W5nC_KhZbf"
      },
      "outputs": [],
      "source": [
        "# Using different types of Learning rates Policies using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "G4H2bS-Bh0YT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.optim is a package implementing various optimization algorithms.\n",
        "model = torch.nn.Linear(20, 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diwycfUJh3Mp",
        "outputId": "35e45f51-41d4-4fc6-cdeb-84422a726477"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    lr: 0.01\n",
              "    maximize: False\n",
              "    momentum: 0.9\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also provide Per-parameter options while defining Optimizer\n",
        "# We can pass an iterable of dict (dictionary). Each of them will define a separate\n",
        "# parameter group, and should contain a params key, containing a list of parameters\n",
        "# belonging to it.\n",
        "\n",
        "perlayer_optimizer = torch.optim.SGD(\n",
        "    [\n",
        "        {'params': model.base.parameters()},\n",
        "        {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "    ], lr=1e-2, momentum=0.9\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "This means that model.base’s parameters will use the default learning rate of 1e-2,\n",
        "model.classifier’s parameters will use a learning rate of 1e-3.\n",
        "And a momentum of 0.9 will be used for all parameters.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DBin2Q7ujJNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding the base class of Optimizers\n",
        "# CLASS   torch.optim.Optimizer(params, defaults) is base class for all optimizer\n",
        "\"\"\"\n",
        " 1. params (iterable) – an iterable of torch.Tensor s or dict s.\n",
        "                        Specifies what Tensors should be optimized.\n",
        "\n",
        " 2. defaults – (dict): a dict containing default values of optimization options\n",
        "                       (used when a parameter group doesn’t specify them).\n",
        "\"\"\"\n",
        "# For more reference, visit here :\n",
        "# https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer"
      ],
      "metadata": {
        "id": "rw0hJBINltDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How ot adjust learning rate during training process\n",
        "# torch.optim.lr_scheduler provides several methods to adjust the learning rate\n",
        "# based on the number of epochs.  (********important)\n",
        "\n",
        "# Learning rate scheduling should be applied after optimizer’s update.\n",
        "lr_policy_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[5, 10, 20], gamma=0.2)\n",
        "\"\"\"\n",
        "milestones (list): List of epoch indices. Must be increasing.\n",
        "gamma (float): Multiplicative factor of learning rate decay. Default: 0.1.\n",
        "\"\"\"\n",
        "dataset = torch.utils.data.Dataset()  # dataset for training\n",
        "MAX_EPOCH = 50\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "\n",
        "  for data, target in dataset:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(data)\n",
        "    loss = loss_fn(prediction, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  lr_policy_scheduler.step()  # applied after epoch\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Note: Most learning rate schedulers can be called back-to-back (also referred to\n",
        "as chaining schedulers). The result is that each scheduler is applied one after\n",
        "the other on the learning rate obtained by the one preceding it.\n",
        "\"\"\"\n",
        "# for example\n",
        "# lr_policy_scheduler_1.step()\n",
        "# lr_policy_scheduler_2.step()"
      ],
      "metadata": {
        "id": "-LfmZsMTnm-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chained Scheduler\n",
        "model = torch.nn.Linear(20, 1)\n",
        "# here, optimizer uses lr =1 for all groups\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0.9)\n",
        "\n",
        "scheduler_1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.5, total_iters=2)\n",
        "\"\"\"\n",
        "if only scheduler_1 is aplied\n",
        ">>> # lr = 0.5   if epoch == 0\n",
        ">>> # lr = 0.5   if epoch == 1\n",
        ">>> # lr = 1   if epoch >= 2\n",
        "\"\"\"\n",
        "\n",
        "scheduler_2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\"\"\"\n",
        "when only scheduler_2 is applied\n",
        ">>> # lr = 0.9     if epoch == 0\n",
        ">>> # lr = 0.81    if epoch == 1\n",
        ">>> # lr = 0.729    if epoch == 2\n",
        ">>> # lr = 0.6561   if epoch == 3\n",
        ">>> # lr = 0.59049  if epoch == 4\n",
        "... and so on\n",
        "\"\"\"\n",
        "\n",
        "chained_scheduler = torch.optim.lr_scheduler.ChainedScheduler([scheduler_1, scheduler_2])\n",
        "\"\"\"\n",
        "when chained_scheduler is applied\n",
        ">>> # lr = 0.45     if epoch == 0\n",
        ">>> # lr = 0.405    if epoch == 1\n",
        ">>> # lr = 0.729    if epoch == 2\n",
        ">>> # lr = 0.6561   if epoch == 3\n",
        ">>> # lr = 0.59049  if epoch == 4\n",
        "... and so on\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Return last computed learning rate by current scheduler.\n",
        "chained_scheduler.get_last_lr()\n",
        "\n"
      ],
      "metadata": {
        "id": "BzN12ZO6vZIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another important Learning rate schedulers\n",
        "plateau_lr_sh = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "\"\"\"\n",
        "Reduce learning rate when a metric has stopped improving. Models often benefit\n",
        "from reducing the learning rate by a factor of 2-10 once learning stagnates.\n",
        "This scheduler reads a metrics quantity and if no improvement is seen for a\n",
        "‘patience’ number of epochs, the learning rate is reduced.\n",
        "\n",
        "mode : min or max\n",
        "In min mode, lr will be reduced when the quantity monitored has stopped decreasing.\n",
        "In max mode it will be reduced when the quantity monitored has stopped increasing.\n",
        "\n",
        "patience : Number of epochs with no improvement after which learning rate will be reduced.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Warming start learning rate decay\n",
        "# refer to torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"
      ],
      "metadata": {
        "id": "4FxxBCuxC3oD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}